---
title: |
  PES University, Bangalore
  
  Established under the Karnataka Act No. 16 of 2013
subtitle: |
  **UE20CS312 - Data Analytics**
  **Worksheet 2c - Logistic Regression**
  Anushka Hebbar - anushkahebbar@pesu.pes.edu
Name : |
  Renita Kurian, PES1UG20CS331 |
output: 
  pdf_document:
    fig_width: 6
    fig_height: 3
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

### Prerequisites
To download the data required for this worksheet, visit this [Github link](https://github.com/Data-Analytics-UE20CS312/Unit-2-Worksheets/tree/main/2c%20-%20Logistic%20Regression).
Use the following libraries and read the dataset:
```{r import, message=FALSE, warning=FALSE, results=TRUE}
library(tidyverse)
library(InformationValue)
library(ggplot2)
df <- read.csv('got_characters.csv')
```

# The Logit Model

The linear regression techniques discussed so far are used to model the relationship between a quantitative response variable and one or more explanatory variables. When the response variable is categorical, other techniques are more suited to the task of classification.

The **logit model**, or **logistic model** models the probability, $p$, that a dichotomous (binary), dependent variable takes on one of two possible outcomes. It achieves this by setting the natural logarithm of the odds of the response variable, called the *log-odds* or *logit*, as a linear function of the explanatory variables.

$$Z_i = \text{log}\left( \frac{p}{1-p}\right) = \beta_0 + \beta_1x + ... + \beta_nx_n \space \text{ for } \space p \in (0,1)$$

Here, $Z_i$ is the log-odds of the response variable taking on a value with probability $p$. 

**Logistic regression** is an algorithm that estimates the parameters, or coefficients, of the linear combination of the logit model. In this worksheet, we will classify a certain binary feature of a dataset using logistic regression.

# A Song of Ice and Fire - GoT Character Dataset

_A Song of Ice and Fire_ by George RR Martin is a series of epic fantasy novels that is popularly known for its TV show adaptation, _Game of Thrones_. The show is well known for its vastly complicated political landscape, large number of characters, and its frequent character deaths.

The given dataset contains comprehensive information on characters from the book series till the 5th book, _A Dance with Dragons_. The data was created by the team at [A Song of Ice and Data](https://got.show/machine-learning-algorithm-predicts-death-game-of-thrones) who scraped it from [the Wiki of Ice and Fire](http://awoiaf.westeros.org/). 

This worksheet will focus on using Logistic Regression to predict whether a character in the SoIaF world remains alive by the end of the 5th book, which is captured by the feature `actual`. 

### Data Dictionary

	actual - Whether the character is alive in the consequent books 
			(0 if dead, 1 if alive)
	name - Name of the character
	title - Character's title
	male - Gender of the character (1 if male, 0 otherwise)
	culture - Culture the character is from
	dateofBirth - Character's DoB
	mother - Name of the character's mother
	father - Name of the character's father
	heir - Name of the character's heir
	spouse - Name of the character's spouse
	book1 - Whether the character appears in Book 1, Game of Thrones
	book2 - Whether the character appears in Book 2, A Clash of Kings
	book3 - Whether the character appears in Book 3, A Storm of Swords
	book4 - Whether the character appears in Book 4, A Feast for Crows
	book5 - Whether the character appears in Book 5, A Dance with Dragons
	isAliveMother - Whether the character's mother is alive
	isAliveFather - Whether the character's father is alive
	isAliveHeir - Whether the character's heir is alive
	isAliveSpouse - Whether the character's spouse is alive
	isMarried - Whether the character is married
	isNoble - Whether the character belongs to a noble family
	boolDeadRelations - Whether one of the character's relations is dead
	numDeadRelations - Count of the character's relations that are dead
	isPopular - Whether the character is popular 
	popularity - Score of the character's popularity

## Points

The problems for this part of the worksheet are for a total of 10 points, with a non-uniform weightage.

- _Problem 1_ : 1 point
- _Problem 2_ : 2 points
- _Problem 3_ : 2 points
- _Problem 4_ : 3 points
- _Problem 5_ : 2 points


# Problems


### Problem 1 (1 point)
How many characters from the SoIaF world does this dataset contain information on? 
Calculate the percentage of missing data for each column of the dataset and print them out in descending order as a dataframe. 
*Hint:* Make sure to capture data from both missing values in numeric fields and empty strings in descriptive fields. Convert all missing placeholders to type NA.
```{r}
nrow(df)
```
This dataset contains information on 1946 characters for SoFaI

```{r}
# Convert empty values to null
library(dplyr)

df1<-df %>%
  mutate_all(~replace(., . == '', NA))
head(df1)
```

```{r}
# Calculate % of missing values
vals<-c(sum(is.na(df1$X)), sum(is.na(df1$S.No)), sum(is.na(df1$actual)), sum(is.na(df1$name)), sum(is.na(df1$title)), sum(is.na(df1$male)), sum(is.na(df1$culture)), sum(is.na(df1$dateOfBirth)), sum(is.na(df1$mother)), sum(is.na(df1$father)), sum(is.na(df1$heir)), sum(is.na(df1$house)), sum(is.na(df1$spouse)), sum(is.na(df1$book1)), sum(is.na(df1$book2)), sum(is.na(df1$book3)), sum(is.na(df1$book4)), sum(is.na(df1$book5)), sum(is.na(df1$isAliveMother)), sum(is.na(df1$isAliveFather)), sum(is.na(df1$isAliveHeir)), sum(is.na(df1$isAliveSpouse)), sum(is.na(df1$isMarried)), sum(is.na(df1$isNoble)), sum(is.na(df1$age)), sum(is.na(df1$numDeadRelations)), sum(is.na(df1$boolDeadRelations)), sum(is.na(df1$isPopular), sum(is.na(df1$popularity))), 0 )
missingVal<- data.frame(cols = colnames(df1), vals = vals)
mv<-transform(missingVal,  percent = format(round((vals/1946)*100, 2), nsmall = 2) ) 
mv<-mv[order(mv$vals, decreasing = TRUE), ]
mv
```
Above table shows the percentage and no. of missing values for each column in decreasing order. It's seen that mother, isAliveMother, heir, isAliveHeir have the highest % of missing values.



### Problem 2 (2 points)
#### Step 1
What are the inferences you can draw from the output dataframe of the previous problem? How can we handle columns with extremely high proportions of missing data before moving on?
*Hint:* Does missing data in a column tell you something about the target variable (`actual`)? If not, set a missing percentage threshold of 80%, deeming the column as having insufficient data, and drop the column. 

In case of columns with extremely high proportion of missing data, the column may be dropped, or column values could be imputed or predicted.

```{r}
# Display columns with missing values percent > 80.00
mv[mv$percent > 80.00, ]
```
The above displayed columns have missing values percentage above 80% and can be dropped.


```{r}
data = subset(df1, select = -c(mother, isAliveMother, heir, isAliveHeir, father, isAliveFather, spouse, isAliveSpouse))
head(data)
```


#### Step 2
Impute missing data in the remaining numeric features with a reasonable statistic. Make sure you observe the distribution of the data in the columns to pick out a reasonable statistic. For categorical variables, convert the columns to numeric features. _Hint: Use the `unclass` function and impute missing categorical feature values with a `-1`._
```{r}
# Distribution of Age - without imputation
ggplot(data, aes(x=age)) +
    geom_histogram(binwidth=5, colour="black", fill="white")
```
```{r}
# Imputation with median
data$age[is.na(data$age)]<-median(data$age, na.rm = TRUE)
ggplot(data, aes(x=age)) + geom_histogram(binwidth=5, colour="black", fill="white")
```
```{r}
#data$age[is.na(data$age)]<-mean(data$age, na.rm = TRUE)
#ggplot(data, aes(x=age)) + geom_histogram(binwidth=5, colour="black", fill="white")
```


```{r}
# Converting categorical data to numeric
data$culture<-unclass(as.factor(data$culture))
data$culture[is.na(data$culture)]<- (-1)

data$name<-unclass(as.factor(data$name))

data$house<-unclass(as.factor(data$house))
data$house[is.na(data$house)]<- (-1)

data$title<-unclass(as.factor(data$title))
data$title[is.na(data$title)]<- (-1)
```


#### Bonus 
After plotting the `age` column, do you notice any discrepancies in the graph? What do you think might have given rise to a such a distribution?
    Since there is a very high proportion of missing values in age column, imputing it with any arbitrary value or mean/median results in drastic change in distribution as all missing values will be replaced with the chosen constant value. Hence, in this case all missing values will be replaced with the median.



### Problem 3 (2 points)
#### Step 1: Check for Class Bias 
Ideally, the proportion of both classes of the target variable should be the same. Is this so in the case of the target variable in this dataset? 
```{r}
table(data$actual)
```
The proportion of both classes for target value is not the same. The odds of a character dying is much smaller.


#### Step 2: Create Training and Test Samples 
Perform undersampling in case of a class bias in the dataset. Create train and test splits. 
_Hint: To create the training sample set, sample 70% of the class with lesser rows and sample the same number from the other class. Use the remaining rows from both classes to create the test split._
```{r}
# i1 - alive characters, i0 - dead characters
i1 <- data[which(data$actual == 1), ]
i0 <- data[which(data$actual == 0), ]

# Select 70 percent of rows of dead class and equal no of alive rows
t1<-sample(1:nrow(i1), 0.7*nrow(i0))
t0<-sample(1:nrow(i0), 0.7*nrow(i0))
training_alive <- i1[t1, ]  
training_dead <- i0[t0, ]
# Combine dead and alive characters and then shuffle
training_data <- rbind(training_alive, training_dead)
training_data <- training_data[sample(1:nrow(training_data)), ]

# test
test_alive <- i1[-t1, ]
test_dead<-i0[-t0, ]
test_data<-rbind(test_alive, test_dead)
test_data<-test_data[sample(1:nrow(test_data)), ]
#Check the distribution of classes in the splits
table(training_data$actual)
table(test_data$actual)
```
```{r}
data
```



### Problem 4 (3 points)
#### Step 1: Build the Logisitic Regression Model 
Train a logistic regression model to predict whether a character is dead by Book 5 (feature: `actual`) using the training split. Use the `summary` function to print the beta coefficients, p values and other statistics. Predict characters' deaths on the test split available.
```{r}
model<-glm(actual ~ age + culture + male + book1 + book2 + book3 + book4 +
                  isMarried + boolDeadRelations + isPopular + popularity, 
                family = binomial(link="logit"), data = training_data)
```
```{r}
summary(model)
```
```{r}
# Predicting for test data
predicted <- plogis(predict(model, test_data))
```


#### Step 2: Decide on the Optimal Prediction Probability Cutoff
The default cutoff prediction probability score is 0.5 or the ratio of 1’s and 0’s in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the training and test samples. Compute the optimal cutoff score (using the test split) that minimizes the misclassification error for the trained model.

_Hint: Use a function from the InformationValue library to perform this task._
```{r}
optCutOff <- optimalCutoff(test_data$actual, predicted)[1] 
optCutOff
```



### Problem 5 (2 points)
Using the optimal cutoff probability, compute and print the following using the predictions on the test set: 
1. Misclassification error
2. Confusion Matrix
3. Sensitivity
4. Specificity

Plot the ROC Curve (Receiver Operating Characteristics Curve). What is the area under the curve?
_Hint: Use predefined functions for this problem._
```{r}
misClassError(test_data$actual, predicted, threshold = optCutOff)
sensitivity(test_data$actual, predicted, threshold = optCutOff)
specificity(test_data$actual, predicted, threshold = optCutOff)
# The columns are actual values (ground truth), while rows are predicted values.
confusionMatrix(test_data$actual, predicted, threshold = optCutOff)
```

```{r}
#Plot the RoC curve and report the AUC
plotROC(test_data$actual, predicted)

```

